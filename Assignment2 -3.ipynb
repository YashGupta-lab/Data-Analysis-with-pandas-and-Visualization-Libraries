{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce52e1f9-a442-4dbf-b7e4-3bac3d655a9b",
   "metadata": {},
   "source": [
    "# Name:- Yash Gupta\n",
    "# Roll NO. :- 2301560069\n",
    "# Course :- MCA sec:-A\n",
    "# Subject :- AIML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bcbd8b",
   "metadata": {},
   "source": [
    "# Deduplicating data\n",
    "\n",
    "In this notebook, we deduplicate data using the [Dedupe library](https://dedupe.readthedocs.io/en/latest/), which utilizes a shallow neural network to learn from a small training exercise.\n",
    "\n",
    "If you are interested in building your own parser, the same folks have created the [Parserator](https://github.com/datamade/parserator) which you can use to extract text features and train your own text extraction (hooray! less brittle than regex!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af172f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dedupe\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('../data/customer_data_duped.csv', \n",
    "                        encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e645b",
   "metadata": {},
   "source": [
    "## Checking Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab234ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b601a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ce873",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in customers.columns:\n",
    "    print(col, customers[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0e980",
   "metadata": {},
   "source": [
    "## Setting up Dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    {'field': 'name', 'type': 'String'},\n",
    "    {'field': 'job', 'type': 'String'},\n",
    "    {'field': 'company', 'type': 'String'},  \n",
    "    {'field': 'street_address','type': 'String'},\n",
    "    {'field': 'city','type': 'String'},\n",
    "    {'field': 'state', 'type': 'String', 'has_missing': True},\n",
    "    {'field': 'email', 'type': 'String', 'has_missing': True},\n",
    "    {'field': 'user_name', 'type': 'String'},\n",
    "]\n",
    "\n",
    "deduper = dedupe.Dedupe(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94adcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23616828",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6e664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deduper.sample(customers.T.to_dict(), 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176ebc3",
   "metadata": {},
   "source": [
    "Note: If you receive an error like this:\n",
    "\n",
    "```/usr/local/lib/python2.7/site-packages/dedupe/sampling.py:39: UserWarning: 250 blocked samples were requested, but only able to sample 249\n",
    "  % (sample_size, len(blocked_sample)))\n",
    "```\n",
    "\n",
    "you can continue (some were selected), or use the suggested number (^ here it would be 249)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde05be",
   "metadata": {},
   "source": [
    "#### Either use training file (uncomment) or resume active training below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e21294",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = '../data/ignore-dedupe-training.json'\n",
    "#if os.path.exists(training_file):\n",
    "#    with open(training_file, 'rb') as f:\n",
    "#        deduper.readTraining(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupe.consoleLabel(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f27df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_file, 'w') as tf:\n",
    "    deduper.writeTraining(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = deduper.match(customers.T.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.iloc[[741,1107]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63d902",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Exercise: Flag duplicates by adding 2 extra columns, one for confidence score and one for duplicate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ebd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/dedupe.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be9627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24bf3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[customers.confidence.notnull() == True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fcdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bbef973",
   "metadata": {},
   "source": [
    "# String Matching\n",
    "\n",
    "In this notebook, we use [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy), a popular string matching library by SeatGeek. \n",
    "\n",
    "For more information on the different methods available and how they differ, see [their blog post explaining methodologies](http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887f0de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e41b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin = ['Berlin, Germany', \n",
    "          'Berlin, Deutschland', \n",
    "          'Berlin', \n",
    "          'Berlin, DE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db42953",
   "metadata": {},
   "source": [
    "#### Try matching the first and second strings: 'Berlin, Germany' and 'Berlin, Deutschland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d04492",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.partial_ratio(berlin[0], berlin[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cdfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75259127",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(berlin[0], berlin[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_set_ratio(berlin[0], berlin[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_sort_ratio(berlin[0], berlin[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62857c1",
   "metadata": {},
   "source": [
    "#### Try matching the second and third strings: 'Berlin, Deutschland' and 'Berlin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea58682",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.partial_ratio(berlin[1], berlin[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95324b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(berlin[1], berlin[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6905902",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_sort_ratio(berlin[1], berlin[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c42de",
   "metadata": {},
   "source": [
    "### What do you think will score lowest and highest for the final two elements: \n",
    "- 'Berlin'\n",
    "- 'Berlin, DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e06c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_set_ratio(berlin[2], berlin[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af17721",
   "metadata": {},
   "source": [
    "### Extracting a guess out of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c56778",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = ['Germany', 'Deutschland', 'France', \n",
    "           'United Kingdom', 'Great Britain', \n",
    "           'United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('DE', choices, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9921307",
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('UK', choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('frankreich', choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b282097",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Will this properly extract?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('U.S.', choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499cdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85ee2916",
   "metadata": {},
   "source": [
    "# Managing Nulls with Pandas\n",
    "\n",
    "In this notebook, we will take a look at some ways to manage nulls using Pandas DataFrames.\n",
    "\n",
    "For even more details on how to do this, check out the [Panda's documentation](http://pandas.pydata.org/pandas-docs/stable/missing_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/iot_example_with_nulls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf9464",
   "metadata": {},
   "source": [
    "### Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f22cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.note.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa7371",
   "metadata": {},
   "source": [
    "### Let's remove all null values (including the note: n/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f141012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/iot_example_with_nulls.csv', \n",
    "                 na_values=['n/a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47bfbd",
   "metadata": {},
   "source": [
    "### Test to see if we can use dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='all', axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba366e2f",
   "metadata": {},
   "source": [
    "### Test to see if we can drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.dropna(thresh=int(df.shape[0] * .9), axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9372806",
   "metadata": {},
   "source": [
    "### I want to find all columns that have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70322100",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_info = list(df.columns[df.isnull().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc48c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in missing_info:\n",
    "    num_missing = df[df[col].isnull() == True].shape[0]\n",
    "    print('number missing for column {}: {}'.format(col, \n",
    "                                                    num_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46766bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in missing_info:\n",
    "    percent_missing = df[df[col].isnull() == True].shape[0] / df.shape[0]\n",
    "    print('percent missing for column {}: {}'.format(\n",
    "        col, percent_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff82d0d",
   "metadata": {},
   "source": [
    "### Can I easily substitute majority values in for missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.note.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.build.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.latest.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b20276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.latest = df.latest.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad73e71",
   "metadata": {},
   "source": [
    "### Have not yet addressed temperature missing values... Let's find a way to fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.username.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d535e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.temperature = df.groupby('username').temperature.fillna(\n",
    "    method='backfill', limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f90e6",
   "metadata": {},
   "source": [
    "### Exercise: How many temperature values did I fill? What percentage of values are still missing (for temperature)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/nulls.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a150a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f674daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "still_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fa665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf1fb629",
   "metadata": {},
   "source": [
    "# Preprocessing with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e367c0",
   "metadata": {},
   "source": [
    "# Scikit Learn Preprocessing\n",
    "\n",
    "In this notebook, we'll use `sklearn.preprocessing` to do some scaling for us. If you need to prepare data for machine learning or feature extraction, the [sklearn.preprocessing documentation](http://scikit-learn.org/stable/modules/preprocessing.html) has great examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3583114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac = pd.read_csv('../data/HVAC_with_nulls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392a946",
   "metadata": {},
   "source": [
    "## Checking Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816fa5d1",
   "metadata": {},
   "source": [
    "## Impute missing values with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded08839",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = preprocessing.Imputer(missing_values='NaN', \n",
    "                            strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f463fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac_numeric = hvac[['TargetTemp', 'SystemAge']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = imp.fit(hvac_numeric.loc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = imp.fit_transform(hvac_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ed866",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac['TargetTemp'], hvac['SystemAge'] = transformed[:,0], transformed[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac71166",
   "metadata": {},
   "source": [
    "## Scale temperature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac['ScaledTemp'] = preprocessing.scale(hvac['ActualTemp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvac['ScaledTemp'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0ff0a",
   "metadata": {},
   "source": [
    "## Scale using a min and max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21568b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad510fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_minmax = min_max_scaler.fit_transform(hvac[['ActualTemp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7debab",
   "metadata": {},
   "source": [
    "### Exercise: add the `temp_minmax` back to the dataframe as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/preprocessing.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61833779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a5487b",
   "metadata": {},
   "source": [
    "# Case Study: Preparing Lobste.rs Stories for Machine Learning\n",
    "\n",
    "In this case study, we'll be preparing [lobste.rs](http://lobste.rs) stories for machine learning. To do so, we need to extract features and clean up the messy parts of the data. We'll be using Pandas along with `sklearn.preprocessing` and `fuzzywuzzy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7079d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32378dc",
   "metadata": {},
   "source": [
    "### If you'd rather read from the API to get the latest, uncomment the details (and add comment to the final line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a978b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resp = requests.get('https://lobste.rs/hottest.json')\n",
    "#stories = pd.read_json(resp.content)\n",
    "#stories = stories.set_index('short_id')\n",
    "\n",
    "stories = pd.read_json('../data/all_lobsters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94df77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97fb0ed",
   "metadata": {},
   "source": [
    "### Let's take a look at the submitter_user field, as it appears like a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd879e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.submitter_user.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = stories['submitter_user'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c763fda",
   "metadata": {},
   "source": [
    "### Can we combine the user data without potential column overlap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641eedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(user_df.columns).intersection(stories.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.rename(columns={'created_at': \n",
    "                                  'user_created_at'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.concat([stories.drop(['submitter_user'], axis=1), \n",
    "                     user_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42583a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6dfe98",
   "metadata": {},
   "source": [
    "### Let's check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdf61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.dropna(thresh=10, axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828969b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Exercise: which columns would be dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/lobsters_dropped.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e297f58",
   "metadata": {},
   "source": [
    "## Let's make the tags easier to use by having them as features in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df = stories.tags.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are our unique tags?\n",
    "\n",
    "pd.unique(tag_df.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tag_df.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(tag_df.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common tags\n",
    "\n",
    "Counter(tag_df.values.ravel()).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa02f9",
   "metadata": {},
   "source": [
    "### Let's create a dummy df with our tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20df0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df = pd.get_dummies(\n",
    "    tag_df.apply(pd.Series).stack()).sum(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfd6c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacc43d",
   "metadata": {},
   "source": [
    "### Now we can add it back to our stories DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.concat([stories.drop('tags', axis=1), \n",
    "                     tag_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d1ffc",
   "metadata": {},
   "source": [
    "### Another potentially useful feature is the post times..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories['created_hour'] = stories.created_at.map(\n",
    "    lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories['created_dow'] = stories.created_at.map(\n",
    "    lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb313ec3",
   "metadata": {},
   "source": [
    "### Let's analyze some of the correlations in our features so far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d90170",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories[['created_hour', 'score']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351febff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories[['created_dow', 'score']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4320cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories[['karma', 'score']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96210d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories[['comment_count', 'score']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d97d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories[['hardware', 'score']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a09efb",
   "metadata": {},
   "source": [
    "## Exercise: can you find a more highly positive correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53746d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "811f4f0f",
   "metadata": {},
   "source": [
    "## We might also want/need to normalize scores. We can use a Scaler / MinMaxScaler or Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_score = preprocessing.normalize(stories[['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_score[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a09b3",
   "metadata": {},
   "source": [
    "#### hmm... maybe a min-max scaler works better for our needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7189e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_score = scaler.fit_transform(stories[['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_score[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories['scaled_score'] = scaled_score[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840b085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77643440",
   "metadata": {},
   "source": [
    "# Dask Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370d7c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## What else should we add?\n",
    "\n",
    "- fuzzywuzzy to find match of title with topics\n",
    "- add normalization or scaling to comments\n",
    "- extract domain name\n",
    "- number of words in the title\n",
    "- number of capitalized words in the title\n",
    "- use NLP to extract named entities from the title\n",
    "- what else?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5deba",
   "metadata": {},
   "source": [
    "## Tracking the International Space Station with Dask\n",
    "\n",
    "In this notebook, we will use two APIs: [Google Maps Geocoder](https://developers.google.com/maps/documentation/geocoding/) and the [open notify API for ISS location](http://api.open-notify.org/). We will use them to track the ISS location and next pass time in relation to a list of cities.\n",
    "\n",
    "To help build our graphs and intelligently parallelize data, we will use [Dask](http://dask.pydata.org/en/latest/), specifically [Dask delayed](http://dask.pydata.org/en/latest/delayed.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8dbfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from math import radians\n",
    "from dask import delayed\n",
    "from operator import itemgetter\n",
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd68b0c",
   "metadata": {},
   "source": [
    "### First, we need to get lat and long pairs from a list of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_long(address):\n",
    "    resp = requests.get(\n",
    "        'https://eu1.locationiq.org/v1/search.php',\n",
    "        params={'key': '92e7ba84cf3465', #Please be kind, you can generate your own for more use here - https://locationiq.org :D\n",
    "                'q': address,\n",
    "                'format': 'json'}\n",
    "    )\n",
    "    if resp.status_code != 200:\n",
    "        print('There was a problem with your request!')\n",
    "        print(resp.content)\n",
    "        return\n",
    "    data = resp.json()[0]\n",
    "    return {\n",
    "        'name': data.get('display_name'),\n",
    "        'lat': float(data.get('lat')),\n",
    "        'long': float(data.get('lon')),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5035674",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lat_long('Berlin, Germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf493eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "for city in ['Seattle, Washington', 'Miami, Florida', \n",
    "             'Berlin, Germany', 'Singapore', \n",
    "             'Wellington, New Zealand',\n",
    "             'Beirut, Lebanon', 'Beijing, China', 'Nairobi, Kenya',\n",
    "             'Cape Town, South Africa', 'Buenos Aires, Argentina']:\n",
    "    locations.append(get_lat_long(city))\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d034d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee504c6",
   "metadata": {},
   "source": [
    "### Now we can define the functions we will use to get the ISS data and compare location and next pass times amongst cities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spaceship_location():\n",
    "    resp = requests.get('http://api.open-notify.org/iss-now.json')\n",
    "    location = resp.json()['iss_position']\n",
    "    return {'lat': float(location.get('latitude')),\n",
    "            'long': float(location.get('longitude'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f59279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def great_circle_dist(lon1, lat1, lon2, lat2):\n",
    "    \"Found on SO: http://stackoverflow.com/a/41858332/380442\"\n",
    "    dist = DistanceMetric.get_metric('haversine')\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    X = [[lat1, lon1], [lat2, lon2]]\n",
    "    kms = 6367\n",
    "    return (kms * dist.pairwise(X)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iss_dist_from_loc(issloc, loc):\n",
    "    distance = great_circle_dist(issloc.get('long'), \n",
    "                                 issloc.get('lat'), \n",
    "                                 loc.get('long'), loc.get('lat'))\n",
    "    logging.info('ISS is ~%dkm from %s', int(distance), loc.get('name'))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iss_pass_near_loc(loc):\n",
    "    resp = requests.get('http://api.open-notify.org/iss-pass.json',\n",
    "                        params={'lat': loc.get('lat'), \n",
    "                                'lon': loc.get('long')})\n",
    "    data = resp.json().get('response')[0]\n",
    "    td = datetime.fromtimestamp(data.get('risetime')) - datetime.now()\n",
    "    m, s = divmod(int(td.total_seconds()), 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    logging.info('ISS will pass near %s in %02d:%02d:%02d',loc.get('name'), h, m, s)\n",
    "    return td.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iss_dist_from_loc(get_spaceship_location(), locations[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40962a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "iss_pass_near_loc(locations[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011d1c2",
   "metadata": {},
   "source": [
    "### Let's create a delayed pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for loc in locations:\n",
    "    issloc = delayed(get_spaceship_location)()\n",
    "    dist = delayed(iss_dist_from_loc)(issloc, loc)\n",
    "    output.append((loc.get('name'), dist))\n",
    "\n",
    "closest = delayed(lambda x: sorted(x, \n",
    "                                   key=itemgetter(1))[0])(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dda740",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7d24e",
   "metadata": {},
   "source": [
    "### Let's see our DAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4c918",
   "metadata": {},
   "source": [
    "### Remember: it is lazy, so let's start it with `compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1862e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decf780",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Exercise: which city will it fly over next?\n",
    "\n",
    "### Extra: add your city and compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fcd722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/dask.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c57914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
